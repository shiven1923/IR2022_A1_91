{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "a3170473-dea6-4480-a7e9-29b11e400b74",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3b1c7f8e",
    "execution_start": 1646303607533,
    "execution_millis": 1416,
    "deepnote_cell_type": "code"
   },
   "source": "import pandas as pd\nimport numpy as np\nimport os\nimport shutil\nfrom pathlib import Path\nimport nltk\nimport string\nimport json\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport pickle",
   "outputs": [
    {
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nThe function [process_query] processes the query as follows:\n1. lowercase the query\n2. remove punctuation\n3. tokenize the query\n4. stopword removal (english stopword removal)\n5. remove blankspace tokens\n\nArguments : Query (String)\nReturns : Query Tokens (List)\n\"\"\"\n\ndef process_query(query):\n    query = query.lower()\n    query = query.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(query)\n    tokens_without_sw = [word for word in tokens if not word in stopwords.words('english')]\n    return tokens_without_sw\n\n#......",
   "metadata": {
    "cell_id": "7ebdd967-d576-4547-ad13-c4677b902376",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1488a8d9",
    "execution_start": 1646303608953,
    "execution_millis": 14,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nThe following code reads the positional_index that was already created and\nstored before. It also reads the mapping from document ids to document name.\nThe data is stored in the form of bytes by the pickle library.\n\"\"\"\n\nf = open(\"positional_index.txt\",\"rb\")\ntxt = f.read()\nf.close()\npositional_index = pickle.loads(txt)\n\nf = open(\"id_to_name.txt\",\"rb\")\ntxt = f.read()\nf.close()\nid_to_name = pickle.loads(txt)\n\n#......",
   "metadata": {
    "cell_id": "be9e1e25-9487-42ef-bd43-730bc35201f9",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f35d326e",
    "execution_start": 1646303609007,
    "execution_millis": 1784,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nThe function [doc_contains_phrase] checks whether a particular document contains\nphrase or not. \n\nArguments : The positions at which term1 occurs in document (List)\n            The positions at which term2 occurs in document (List)\nReturns : True if phrase occurs in document, False if not (Boolean)\n\n\"\"\"\ndef doc_contains_phrase(postion_term1, position_term2):\n    for i in postion_term1:\n        if (i+1) in position_term2:\n            return True\n    return False\n\n#......",
   "metadata": {
    "cell_id": "b9486876-f54d-44de-88f1-285ac4393b4b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d9a2e9f",
    "execution_start": 1646303610799,
    "execution_millis": 5,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nThe function [phrase_retrieval] is the core function for phrase retrieval.\n\nArguments : Query (String)\n            Positional_Index (Dictionary)\nReturns : List of documents (names) containing the query (List)\n\nThe function works the following way:\n1. processes the query with the help of [process_query] function to get tokens.\n2. gets the list of all eligible documents for first token (token1)\n3. gets the list of all eligible documents for second token (token2)\n4. intersect the two lists to get common documents\n5. iterate through each of the common document to check if document contains\n   token1 + token2 in it. The helper function [doc_contains_phrase] is used.\n6. a new list is created containing documents having token1 + token2. \n7. token1 is assigned the value token2 and token2 then takes the value of next\n   token i.e third token (token3)\n8. steps 3 to 7 are repeated until tokens are finished\nCommon List is the Final Answer after all the above steps\n\"\"\"\n\n\ndef phrase_retrieval(query,positional_index):\n    query_tokens = process_query(query)\n    eligible = []\n    n = len(query_tokens)\n    if (n==0):\n        return []\n    query1 = query_tokens[0]\n    try:\n        eligible = positional_index[query1].copy()\n    except KeyError:\n        eligible = {}\n    list_of_docs1 = np.array(list(eligible.keys()))\n    for query in query_tokens[1:]:\n        query2 = query\n        try:\n            list_of_docs2 = np.array(list(positional_index[query2].keys()))\n        except KeyError:\n            list_of_docs2 = np.array([])\n        common_docs = list(np.intersect1d(list_of_docs1,list_of_docs2))\n        common_docs_new = []\n        for i in common_docs:\n            try:\n                position_term1 = positional_index[query1][i]\n            except KeyError:\n                position_term1 = []\n\n            try:\n                position_term2 = positional_index[query2][i]\n            except KeyError:\n                position_term2 = []\n\n            if doc_contains_phrase(position_term1,position_term2):\n                common_docs_new.append(i)\n        list_of_docs1 = np.array(common_docs_new)\n        query1 = query2\n    \n    common_docs_with_name = []\n    for i in list(list_of_docs1):\n        common_docs_with_name.append(id_to_name[i])\n    return common_docs_with_name\n\n#.....",
   "metadata": {
    "cell_id": "7b4577cb-a1cf-4481-a640-bf11b6a008e4",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7f8fb577",
    "execution_start": 1646303610824,
    "execution_millis": 4,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "while True:\n    query = input(\"Enter Phrase Query\")\n    documents = phrase_retrieval(query,positional_index)\n    print(\"Number of Documents Retrieved :\",len(documents))\n    print(\"The Documents containing (\"+query+\") are:\")\n    for i in documents:\n        print(i)\n",
   "metadata": {
    "cell_id": "e111c493-fe0b-4ec0-8bd3-7387f27e178e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c289431c",
    "execution_start": 1646320497877,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=04d84d63-ff1d-4672-9f78-d034a2868658' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "4c311511-d74b-4171-afb7-1e64f8c59114",
  "deepnote_execution_queue": []
 }
}